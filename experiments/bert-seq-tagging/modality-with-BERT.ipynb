{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "np.random.seed(3)\n",
    "torch.cuda.manual_seed_all(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ix</th>\n",
       "      <th>token</th>\n",
       "      <th>is_modal</th>\n",
       "      <th>is_prej</th>\n",
       "      <th>modal_type</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#</td>\n",
       "      <td>Sent_number =</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#</td>\n",
       "      <td>sentence_text =</td>\n",
       "      <td>The American Department of State , in its annu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#</td>\n",
       "      <td>modal_count =</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#</td>\n",
       "      <td>source_document =</td>\n",
       "      <td>02.54-18922</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>138</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>139</td>\n",
       "      <td>American</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>140</td>\n",
       "      <td>Department</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>141</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>142</td>\n",
       "      <td>State</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>144</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>145</td>\n",
       "      <td>its</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>146</td>\n",
       "      <td>annual</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>147</td>\n",
       "      <td>report</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>148</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34</td>\n",
       "      <td>human</td>\n",
       "      <td>O</td>\n",
       "      <td>S</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>modal_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>rights</td>\n",
       "      <td>S-priority</td>\n",
       "      <td>_</td>\n",
       "      <td>priority:deontic</td>\n",
       "      <td>0</td>\n",
       "      <td>modal_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>149</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>150</td>\n",
       "      <td>has</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>151</td>\n",
       "      <td>accused</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>152</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>153</td>\n",
       "      <td>number</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>154</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>155</td>\n",
       "      <td>countries</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>156</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>157</td>\n",
       "      <td>including</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>158</td>\n",
       "      <td>Iran</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>159</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>160</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355488</th>\n",
       "      <td>455</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355489</th>\n",
       "      <td>456</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355490</th>\n",
       "      <td>457</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355491</th>\n",
       "      <td>458</td>\n",
       "      <td>Mali</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355492</th>\n",
       "      <td>459</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355493</th>\n",
       "      <td>460</td>\n",
       "      <td>Greece</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355494</th>\n",
       "      <td>461</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355495</th>\n",
       "      <td>462</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355496</th>\n",
       "      <td>463</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11047</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355497</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355498</th>\n",
       "      <td>#</td>\n",
       "      <td>Sent_number =</td>\n",
       "      <td>11048</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355499</th>\n",
       "      <td>#</td>\n",
       "      <td>sentence_text =</td>\n",
       "      <td>The committee will meet again Thursday to cont...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355500</th>\n",
       "      <td>#</td>\n",
       "      <td>modal_count =</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355501</th>\n",
       "      <td>#</td>\n",
       "      <td>source_document =</td>\n",
       "      <td>16.57-11242</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355502</th>\n",
       "      <td>464</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355503</th>\n",
       "      <td>465</td>\n",
       "      <td>committee</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355504</th>\n",
       "      <td>466</td>\n",
       "      <td>will</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355505</th>\n",
       "      <td>467</td>\n",
       "      <td>meet</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355506</th>\n",
       "      <td>468</td>\n",
       "      <td>again</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355507</th>\n",
       "      <td>469</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355508</th>\n",
       "      <td>470</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355509</th>\n",
       "      <td>471</td>\n",
       "      <td>continue</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355510</th>\n",
       "      <td>472</td>\n",
       "      <td>its</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355511</th>\n",
       "      <td>473</td>\n",
       "      <td>general</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355512</th>\n",
       "      <td>474</td>\n",
       "      <td>debate</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355513</th>\n",
       "      <td>475</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355514</th>\n",
       "      <td>476</td>\n",
       "      <td>human</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355515</th>\n",
       "      <td>477</td>\n",
       "      <td>rights</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355516</th>\n",
       "      <td>478</td>\n",
       "      <td>questions</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355517</th>\n",
       "      <td>479</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>11048</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355518 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ix               token  \\\n",
       "0                                 \n",
       "1         #      Sent_number =    \n",
       "2         #    sentence_text =    \n",
       "3         #      modal_count =    \n",
       "4         #  source_document =    \n",
       "5       138                 The   \n",
       "6       139            American   \n",
       "7       140          Department   \n",
       "8       141                  of   \n",
       "9       142               State   \n",
       "10      143                   ,   \n",
       "11      144                  in   \n",
       "12      145                 its   \n",
       "13      146              annual   \n",
       "14      147              report   \n",
       "15      148                  on   \n",
       "16       34               human   \n",
       "17       20              rights   \n",
       "18      149                   ,   \n",
       "19      150                 has   \n",
       "20      151             accused   \n",
       "21      152                   a   \n",
       "22      153              number   \n",
       "23      154                  of   \n",
       "24      155           countries   \n",
       "25      156                   ,   \n",
       "26      157           including   \n",
       "27      158                Iran   \n",
       "28      159                   ,   \n",
       "29      160                  of   \n",
       "...     ...                 ...   \n",
       "355488  455                   ,   \n",
       "355489  456           Venezuela   \n",
       "355490  457                   ,   \n",
       "355491  458                Mali   \n",
       "355492  459                   ,   \n",
       "355493  460              Greece   \n",
       "355494  461                 and   \n",
       "355495  462           Indonesia   \n",
       "355496  463                   .   \n",
       "355497                            \n",
       "355498    #      Sent_number =    \n",
       "355499    #    sentence_text =    \n",
       "355500    #      modal_count =    \n",
       "355501    #  source_document =    \n",
       "355502  464                 The   \n",
       "355503  465           committee   \n",
       "355504  466                will   \n",
       "355505  467                meet   \n",
       "355506  468               again   \n",
       "355507  469            Thursday   \n",
       "355508  470                  to   \n",
       "355509  471            continue   \n",
       "355510  472                 its   \n",
       "355511  473             general   \n",
       "355512  474              debate   \n",
       "355513  475                  on   \n",
       "355514  476               human   \n",
       "355515  477              rights   \n",
       "355516  478           questions   \n",
       "355517  479                   .   \n",
       "\n",
       "                                                 is_modal is_prej  \\\n",
       "0                                                                   \n",
       "1                                                       0           \n",
       "2       The American Department of State , in its annu...           \n",
       "3                                                       0           \n",
       "4                                             02.54-18922           \n",
       "5                                                       O       _   \n",
       "6                                                       O       _   \n",
       "7                                                       O       _   \n",
       "8                                                       O       _   \n",
       "9                                                       O       _   \n",
       "10                                                      O       _   \n",
       "11                                                      O       _   \n",
       "12                                                      O       _   \n",
       "13                                                      O       _   \n",
       "14                                                      O       _   \n",
       "15                                                      O       _   \n",
       "16                                                      O       S   \n",
       "17                                             S-priority       _   \n",
       "18                                                      O       _   \n",
       "19                                                      O       _   \n",
       "20                                                      O       _   \n",
       "21                                                      O       _   \n",
       "22                                                      O       _   \n",
       "23                                                      O       _   \n",
       "24                                                      O       _   \n",
       "25                                                      O       _   \n",
       "26                                                      O       _   \n",
       "27                                                      O       _   \n",
       "28                                                      O       _   \n",
       "29                                                      O       _   \n",
       "...                                                   ...     ...   \n",
       "355488                                                  O       _   \n",
       "355489                                                  O       _   \n",
       "355490                                                  O       _   \n",
       "355491                                                  O       _   \n",
       "355492                                                  O       _   \n",
       "355493                                                  O       _   \n",
       "355494                                                  O       _   \n",
       "355495                                                  O       _   \n",
       "355496                                                  O       _   \n",
       "355497                                                              \n",
       "355498                                              11048           \n",
       "355499  The committee will meet again Thursday to cont...           \n",
       "355500                                                  0           \n",
       "355501                                        16.57-11242           \n",
       "355502                                                  O       _   \n",
       "355503                                                  O       _   \n",
       "355504                                                  O       _   \n",
       "355505                                                  O       _   \n",
       "355506                                                  O       _   \n",
       "355507                                                  O       _   \n",
       "355508                                                  O       _   \n",
       "355509                                                  O       _   \n",
       "355510                                                  O       _   \n",
       "355511                                                  O       _   \n",
       "355512                                                  O       _   \n",
       "355513                                                  O       _   \n",
       "355514                                                  O       _   \n",
       "355515                                                  O       _   \n",
       "355516                                                  O       _   \n",
       "355517                                                  O       _   \n",
       "\n",
       "              modal_type sentence_number      span  \n",
       "0                                                   \n",
       "1                                                   \n",
       "2                                                   \n",
       "3                                                   \n",
       "4                                                   \n",
       "5                      _               0         _  \n",
       "6                      _               0         _  \n",
       "7                      _               0         _  \n",
       "8                      _               0         _  \n",
       "9                      _               0         _  \n",
       "10                     _               0         _  \n",
       "11                     _               0         _  \n",
       "12                     _               0         _  \n",
       "13                     _               0         _  \n",
       "14                     _               0         _  \n",
       "15                     _               0         _  \n",
       "16                     _               0  modal_20  \n",
       "17      priority:deontic               0  modal_20  \n",
       "18                     _               0         _  \n",
       "19                     _               0         _  \n",
       "20                     _               0         _  \n",
       "21                     _               0         _  \n",
       "22                     _               0         _  \n",
       "23                     _               0         _  \n",
       "24                     _               0         _  \n",
       "25                     _               0         _  \n",
       "26                     _               0         _  \n",
       "27                     _               0         _  \n",
       "28                     _               0         _  \n",
       "29                     _               0         _  \n",
       "...                  ...             ...       ...  \n",
       "355488                 _           11047         _  \n",
       "355489                 _           11047         _  \n",
       "355490                 _           11047         _  \n",
       "355491                 _           11047         _  \n",
       "355492                 _           11047         _  \n",
       "355493                 _           11047         _  \n",
       "355494                 _           11047         _  \n",
       "355495                 _           11047         _  \n",
       "355496                 _           11047         _  \n",
       "355497                                              \n",
       "355498                                              \n",
       "355499                                              \n",
       "355500                                              \n",
       "355501                                              \n",
       "355502                 _           11048         _  \n",
       "355503                 _           11048         _  \n",
       "355504                 _           11048         _  \n",
       "355505                 _           11048         _  \n",
       "355506                 _           11048         _  \n",
       "355507                 _           11048         _  \n",
       "355508                 _           11048         _  \n",
       "355509                 _           11048         _  \n",
       "355510                 _           11048         _  \n",
       "355511                 _           11048         _  \n",
       "355512                 _           11048         _  \n",
       "355513                 _           11048         _  \n",
       "355514                 _           11048         _  \n",
       "355515                 _           11048         _  \n",
       "355516                 _           11048         _  \n",
       "355517                 _           11048         _  \n",
       "\n",
       "[355518 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gme_df = pd.read_csv('tokenized_and_tagged_gme_coarse_grained.csv', sep='\\t', keep_default_na=False)\n",
    "gme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_train_and_test_sets(df, train_size: float):\n",
    "    sent_numbers = df['sentence_number'].unique()\n",
    "    train_sents, test_sents = sent_numbers[1:(int(len(sent_numbers)*train_size))], sent_numbers[(int(len(sent_numbers)*train_size)):]\n",
    "    train_set, test_set = df[df['sentence_number'].isin(train_sents)], df[df['sentence_number'].isin(test_sents)]\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_to_train_and_test_sets(gme_df, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, dataframe, max_sent=None):\n",
    "        self.df = dataframe\n",
    "        self.tags = self.df['is_modal'].unique().tolist()\n",
    "        self.tags.insert(0,'PAD')\n",
    "        \n",
    "        self.index = 0\n",
    "        self.max_sent = max_sent\n",
    "        self.tokens = dataframe['token']\n",
    "        self.modal_tags = dataframe['is_modal']\n",
    "        \n",
    "    \n",
    "          \n",
    "    def get_tokens_and_tags_by_sentences(self):\n",
    "        sent = []\n",
    "        counter = 0\n",
    "        \n",
    "        for token,tag in zip(self.tokens, self.modal_tags):\n",
    "            sent.append((token, tag))\n",
    "            if token.strip() in ['.', '?', '!']:\n",
    "                yield sent\n",
    "                sent = []\n",
    "                counter += 1\n",
    "            if self.max_sent is not None and counter >= self.max_sent:\n",
    "                return\n",
    "\n",
    "    def get_tag2idx(self):\n",
    "        return {tag:idx for idx, tag in enumerate(self.tags)}\n",
    "            \n",
    "    def get_idx2tag(self):\n",
    "        return {idx:tag for idx, tag in enumerate(self.tags)}\n",
    "    \n",
    "    def get_2Dlist_of_sentences(self):\n",
    "        return [[token for token, tag in sent] for sent in self.get_tokens_and_tags_by_sentences()]\n",
    "    \n",
    "    def get_2Dlist_of_tags(self):\n",
    "        return [[tag for token, tag in sent] for sent in self.get_tokens_and_tags_by_sentences()]\n",
    "        \n",
    "        \n",
    "# train_getter = SentenceGetter(train_df)\n",
    "# test_getter = SentenceGetter(test_df)\n",
    "\n",
    "# train_sentences = train_getter.get_2Dlist_of_sentences()\n",
    "# train_tags = train_getter.get_2Dlist_of_tags()\n",
    "\n",
    "# test_sentences = test_getter.get_2Dlist_of_sentences()\n",
    "# test_tags = test_getter.get_2Dlist_of_tags()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8839, 2210)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def __init__(self, train_df, test_df, pre_trained='bert-base-cased'):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class BertTrainer(object):\n",
    "        \n",
    "    MAX_LEN = 150\n",
    "    bs = 32\n",
    "    \n",
    "    def __init__(self, train_df, test_df, pre_trained='bert-base-cased'):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.train_getter = SentenceGetter(self.train_df)\n",
    "        self.test_getter = SentenceGetter(self.test_df)\n",
    "#         self.train_sentence = self.train_getter.get_2Dlist_of_sentences()\n",
    "#         self.train_tags = self.get_2Dlist_of_tags()\n",
    "        \n",
    "        self.device, self.n_gpu = self.set_cuda()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pre_trained, do_lower_case=False)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def set_cuda(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        return device, n_gpu\n",
    "    \n",
    "    def tokenize(self, sentences, orig_labels):    \n",
    "        tokenized_texts = []\n",
    "        labels = []\n",
    "        sents, tags_li = [], []\n",
    "        for sent, sent_labels in zip(sentences, orig_labels):\n",
    "            bert_tokens = []\n",
    "            bert_labels = []\n",
    "            for orig_token, orig_label in zip(sent, sent_labels):\n",
    "                b_tokens = self.tokenizer.tokenize(orig_token)\n",
    "                bert_tokens.extend(b_tokens)\n",
    "                for b_token in b_tokens:\n",
    "                    bert_labels.append(orig_label)\n",
    "            tokenized_texts.append(bert_tokens)\n",
    "            labels.append(bert_labels)\n",
    "            assert len(bert_tokens) == len(bert_labels)\n",
    "        return tokenized_texts, labels\n",
    "\n",
    "\n",
    "    def pad_sentences_and_labels(self, tokenized_texts, labels):\n",
    "        input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                                  maxlen = MAX_LEN, dtype = \"int\", truncating = \"post\", padding = \"post\")\n",
    "        tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "                             maxlen = MAX_LEN, value = tag2idx['PAD'], padding = \"post\",\n",
    "                            dtype = \"int\", truncating = \"post\")\n",
    "        attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "        return input_ids, tags, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTrainer' object has no attribute 'MAX_LEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a1e08b25e078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_tokenized_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokenized_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sentences_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokenized_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokenized_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-05eb62fa62f8>\u001b[0m in \u001b[0;36mpad_sentences_and_labels\u001b[0;34m(self, tokenized_texts, labels)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpad_sentences_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n\u001b[0;32m---> 43\u001b[0;31m                                   maxlen = self.MAX_LEN, dtype = \"int\", truncating = \"post\", padding = \"post\")\n\u001b[0m\u001b[1;32m     44\u001b[0m         tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n\u001b[1;32m     45\u001b[0m                              \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTrainer' object has no attribute 'MAX_LEN'"
     ]
    }
   ],
   "source": [
    "bert = BertTrainer(train_df, test_df, pre_trained='bert-base-cased')\n",
    "\n",
    "train_sentences, train_tags = bert.train_getter.get_2Dlist_of_sentences(), bert.train_getter.get_2Dlist_of_tags()\n",
    "tag2idx = {**bert.train_getter.get_tag2idx(), **bert.test_getter.get_tag2idx()}\n",
    "idx2tag = {**bert.train_getter.get_idx2tag(), **bert.test_getter.get_idx2tag()}\n",
    "\n",
    "\n",
    "train_tokenized_texts, train_tokenized_labels = bert.tokenize(train_sentence, train_tags)\n",
    "input_ids, tags, attention_masks = bert.pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(\"Device: \" + str(device))\n",
    "print(\"Number of gpus: \" + str(n_gpu))\n",
    "if device == 'cuda':\n",
    "    print(\"Name of gpu: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {**train_getter.get_tag2idx(), **test_getter.get_tag2idx()}\n",
    "idx2tag = {**train_getter.get_idx2tag(), **test_getter.get_idx2tag()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('../resources/wwm_cased_L-24_H-1024_A-16/')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, orig_labels):    \n",
    "    tokenized_texts = []\n",
    "    labels = []\n",
    "    sents, tags_li = [], []\n",
    "    for sent, sent_labels in zip(sentences, orig_labels):\n",
    "        bert_tokens = []\n",
    "        bert_labels = []\n",
    "        for orig_token, orig_label in zip(sent, sent_labels):\n",
    "            b_tokens = tokenizer.tokenize(orig_token)\n",
    "            bert_tokens.extend(b_tokens)\n",
    "            for b_token in b_tokens:\n",
    "                bert_labels.append(orig_label)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "        labels.append(bert_labels)\n",
    "        assert len(bert_tokens) == len(bert_labels)\n",
    "    return tokenized_texts, labels\n",
    "\n",
    "\n",
    "def pad_sentences_and_labels(tokenized_texts, labels):\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen = MAX_LEN, dtype = \"int\", truncating = \"post\", padding = \"post\")\n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "                         maxlen = MAX_LEN, value = tag2idx['PAD'], padding = \"post\",\n",
    "                        dtype = \"int\", truncating = \"post\")\n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "    return input_ids, tags, attention_masks\n",
    "\n",
    "\n",
    "train_tokenized_texts, train_tokenized_labels = tokenize(train_sentence, train_tags)\n",
    "input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForTokenClassification.from_pretrained('../resources/wwm_cased_L-24_H-1024_A-16/', num_labels=len(tag2idx))\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(tag2idx))\n",
    "model.cuda()\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
